{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:30px; text-align:center; line-height:120%\">\n",
    "    <br> \n",
    "        <b>\n",
    "        COMS 4995 Applied ML\n",
    "            Homework 4 \n",
    "        <br></br>\n",
    "            Predicting Wine Quality: Task 2\n",
    "        <br></br>\n",
    "        </b> \n",
    "    <br> \n",
    "</p>\n",
    "<p style=\"font-size:18px; text-align:left; line-height:120%\">\n",
    "    <br> \n",
    "        <b>\n",
    "        Kirit Dhillon, Sagar Lal\n",
    "        </b>\n",
    "    <br> \n",
    "        <b>\n",
    "        Uni: ksd2142, sl3946\n",
    "        </b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"winemag-data-130k-v2.csv\")\n",
    "# Remove uninformative columns like \"Taster Name\" and \"Taster Twitter Handle\"\n",
    "data = data.drop(['taster_name', 'taster_twitter_handle'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data['description'], data['points'], stratify= data['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainval: \t (97478,) (97478,)\n",
      "X_test: \t (32493,) (32493,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trainval: \\t\", X_trainval.shape, y_trainval.shape)\n",
    "print(\"X_test: \\t\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vect = CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    "\n",
    "X_trainval_bow = bow_vect.fit_transform(X_trainval)\n",
    "X_test_bow = bow_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoW Text] X_trainval: \t (97478, 1000)\n",
      "[BoW Text] X_test:\t (32493, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"[BoW Text] X_trainval: \\t\", X_trainval_bow.shape)\n",
    "print(\"[BoW Text] X_test:\\t\", X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Word Embeddings Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We decided to use a pre-trained Doc2vec model called \"Associated Press News DBOW\" which is analogous to a skip-gram model in word2vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph. (Source: https://ibm.ent.box.com/s/9ebs3c759qqo1d8i7ed323i6shv2js7e)\n",
    "- We picked this over a word2vec model because it's far more efficient and computationally inexpensive for a large corpus like our wine review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "import gensim.models as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = g.Doc2Vec.load('./apnews_dbow/doc2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset so it can be processed by the doc2vec model\n",
    "def preprocess(model, dataset):\n",
    "    list_dataset= list(dataset.str.split(\" \", expand = False))\n",
    "    print(\"Successful splitting\")\n",
    "    w2v_dataset = []\n",
    "    j = 0\n",
    "    #print(list_dataset[1])\n",
    "    for i in list_dataset:\n",
    "        j+=1\n",
    "        if j%10000 == 0:\n",
    "            print(j)\n",
    "        w2v_dataset.append(model.infer_vector(i))\n",
    "    return w2v_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful splitting\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "Completed X_test preprocessing\n"
     ]
    }
   ],
   "source": [
    "w2v_x_test = preprocess(model, X_test)\n",
    "print(\"Completed X_test preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful splitting\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "Completed X_trainval preprocessing\n"
     ]
    }
   ],
   "source": [
    "w2v_x_trainval = preprocess(model, X_trainval)\n",
    "print(\"Completed X_trainval preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to Pandas dataframe to allow for concatenation with non-text features later\n",
    "w2v_x_test = pd.DataFrame(w2v_x_test)\n",
    "w2v_x_trainval = pd.DataFrame(w2v_x_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[w2v] X_trainval: \t (97478, 300)\n",
      "[w2v] X_test: \t (32493, 300)\n"
     ]
    }
   ],
   "source": [
    "# Debugging: \n",
    "print(\"[w2v] X_trainval: \\t\", w2v_x_trainval.shape)\n",
    "print(\"[w2v] X_test: \\t\", w2v_x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Ridge score with GridSearchCV 0.45\n",
      "Best param: {'regressor__alpha': 10}\n",
      "CPU times: user 13.5 s, sys: 4.3 s, total: 17.8 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, .1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(w2v_x_trainval, y_trainval)\n",
    "\n",
    "print((\"Doc2Vec Ridge score with GridSearchCV %.2f\"\n",
    "       %grid.score(w2v_x_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', XGBRegressor())])\n",
    "\n",
    "param_grid =  {\n",
    "               \"regressor__max_depth\": [4], \n",
    "                \"regressor__alpha\": [0], \n",
    "                \"regressor__lambda\": [0.5],\n",
    "              }\n",
    "grid = GridSearchCV(xgb_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(w2v_x_trainval, y_trainval)\n",
    "\n",
    "print((\"XGBoost with GridSearchCV %.2f\"\n",
    "       %grid.score(w2v_x_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "- We tried the pre-trained Doc2Vec word embeddings for two models: Ridge and XGBRegressor\n",
    "- XGB regressor failed to train even after waiting for several hours.\n",
    "- Ridge perform signficantly worse than models in task 1 45% accuracy vs 69%.\n",
    "- We believe that pre-trained word embeddings are not appropriate for featurization for this task because the embeddings are trained on AP News article which might not capture the nuance that is specifically around wine vocabulary and the appreciation of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Doc2Vec with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval_bow = pd.DataFrame(X_trainval_bow.toarray())\n",
    "X_test_bow = pd.DataFrame(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X_trainval = pd.concat([X_trainval_bow, w2v_x_trainval], axis=1)\n",
    "full_X_test = pd.concat([X_test_bow, w2v_x_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec + BOW Ridge score with GridSearchCV 0.63\n",
      "Best param: {'regressor__alpha': 10}\n",
      "CPU times: user 2min 12s, sys: 43.8 s, total: 2min 56s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(full_X_trainval, y_trainval)\n",
    "\n",
    "print((\"Doc2Vec + BOW Ridge score with GridSearchCV %.2f\"\n",
    "       %grid.score(full_X_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "- Combining Doc2Vec with BoW for featurization on Ridge performs very well, second only to the text-based N-gram text-based Ridge with TF-IDF stemming by 4% accuracy.\n",
    "- While this improves performance compared to solely Doc2Vec featurization, we find it's still better to avoid pre-trained embeddings altogether."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
