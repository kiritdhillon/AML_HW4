{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:30px; text-align:center; line-height:120%\">\n",
    "    <br> \n",
    "        <b>\n",
    "        COMS 4995 Applied ML\n",
    "            Homework 4 \n",
    "        <br></br>\n",
    "            Predicting Wine Quality: Task 1\n",
    "        <br></br>\n",
    "        </b> \n",
    "    <br> \n",
    "</p>\n",
    "<p style=\"font-size:18px; text-align:left; line-height:120%\">\n",
    "    <br> \n",
    "        <b>\n",
    "        Kirit Dhillon, Sagar Lal\n",
    "        </b>\n",
    "    <br> \n",
    "        <b>\n",
    "        Uni: ksd2142, sl3946\n",
    "        </b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"winemag-data-130k-v2.csv\")\n",
    "# Remove uninformative columns like \"Taster Name\" and \"Taster Twitter Handle\"\n",
    "data = data.drop(['taster_name', 'taster_twitter_handle'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0       int64\n",
      "country         object\n",
      "description     object\n",
      "designation     object\n",
      "points           int64\n",
      "price          float64\n",
      "province        object\n",
      "region_1        object\n",
      "region_2        object\n",
      "title           object\n",
      "variety         object\n",
      "winery          object\n",
      "dtype: object\n",
      "Shape of entire dataset:  (129971, 12)\n"
     ]
    }
   ],
   "source": [
    "# Check data types for each column\n",
    "print(data.dtypes)\n",
    "print(\"Shape of entire dataset: \",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split before deciding between text and non-text data to allow for joining later\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data.drop(['points'], axis=1), data['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Data\n",
    "text_X_trainval = X_trainval['description']\n",
    "text_X_test = X_test['description']\n",
    "# Non-Text Data\n",
    "non_text_X_trainval = X_trainval.drop(['description'], axis=1).reset_index().drop(['index', 'Unnamed: 0'], axis=1)\n",
    "non_text_X_test = X_test.drop(['description'], axis=1).reset_index().drop(['index', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text] X_trainval: \t (97478,) (97478,)\n",
      "[Text] X_test: \t\t (32493,) (32493,)\n",
      "[Non-Text] X_trainval: \t (97478, 9)\n",
      "[Non-Text] X_test: \t (32493, 9)\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"[Text] X_trainval: \\t\", text_X_trainval.shape, y_trainval.shape)\n",
    "print(\"[Text] X_test: \\t\\t\", text_X_test.shape, y_test.shape)\n",
    "print(\"[Non-Text] X_trainval: \\t\", non_text_X_trainval.shape)\n",
    "print(\"[Non-Text] X_test: \\t\", non_text_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Baseline Model using only Non-Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:\n",
    "- Imputation of missing values for both categorical and continuous non-text features\n",
    "- OHE for categorical non-text features, StandardScaler for continuous non-text features\n",
    "\n",
    "#### Model(s) tried: \n",
    "- Ridge with 5-fold cross-validation\n",
    "- Ridge with 5-fold cross-validation and grid search on alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_continuous_features = ['price']\n",
    "nt_categorical_features = ['country', 'title', 'designation', 'province', 'region_1', 'region_2',\n",
    "                        'variety', 'winery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_categorical_transformer = Pipeline(steps=[('simpleimputer', SimpleImputer(strategy = 'constant', fill_value = \"missing\")),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "base_continuous_transformer = Pipeline(steps=[('simpleimputer', SimpleImputer(strategy = \"median\")),\n",
    "                                         ('scaler', StandardScaler())])\n",
    "\n",
    "base_continuous_and_categorical_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', base_continuous_transformer, nt_continuous_features),\n",
    "        ('cat', base_categorical_transformer, nt_categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Ridge with only CV: 0.47\n",
      "CPU times: user 4min 14s, sys: 1.65 s, total: 4min 15s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Baseline Ridge CV only\n",
    "base_ridge_clf = Pipeline(steps=[('preprocessor', base_continuous_and_categorical_preprocessor),\n",
    "                                 ('regressor', Ridge())])\n",
    "\n",
    "base_ridge_scores = cross_val_score(base_ridge_clf, non_text_X_trainval, y_trainval, cv=5)\n",
    "\n",
    "print(\"Baseline Ridge with only CV: %.2f\" % np.mean(base_ridge_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Ridge score with GridSearchCV 0.53\n",
      "Best param: {'regressor__alpha': 1}\n",
      "CPU times: user 5min 9s, sys: 1min 19s, total: 6min 28s\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Baseline Ridge GridSearchCV \n",
    "ridge_pipeline = Pipeline(steps=[('preprocessor', base_continuous_and_categorical_preprocessor),\n",
    "                                ('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=5, return_train_score=True)\n",
    "grid.fit(non_text_X_trainval, y_trainval)\n",
    "\n",
    "print((\"Baseline Ridge score with GridSearchCV: %.2f\"\n",
    "       %grid.score(non_text_X_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "- A baseline ridge model with GridSearchCV performs better (0.53) than a baseline ridge model with just cross-validation (0.47)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Text-based Model based on BOW and a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vect = CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    "\n",
    "X_trainval_bow = bow_vect.fit_transform(text_X_trainval)\n",
    "X_test_bow = bow_vect.transform(text_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoW Text] X_trainval: \t (97478, 1000)\n",
      "[BoW Text] X_test:\t (32493, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"[BoW Text] X_trainval: \\t\", X_trainval_bow.shape)\n",
    "print(\"[BoW Text] X_test:\\t\", X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-based Rigde score with GridSearchCV 0.61\n",
      "Best param: {'regressor__alpha': 10}\n",
      "CPU times: user 19.4 s, sys: 346 ms, total: 19.7 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_trainval_bow, y_trainval)\n",
    "\n",
    "print((\"Text-based Ridge score with GridSearchCV %.2f\"\n",
    "       %grid.score(X_test_bow, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "- A text-based Ridge model with GridSearchCV outperforms (0.61) the baseline models from 1.1 (0.47, 0.53).\n",
    "- Even though a BOW approach discards any information about the order or structure of words in the document, we find that the vocabulary of wine reviews itself is helpful in predicting the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: More text-based models: \n",
    "We decided to explore the following two approaches to tune the BoW model: \n",
    "- Bigram/unigram models with stemming and removing stopwords\n",
    "- Expanding on the n-gram model above using tf-idf stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. N-gram model with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Citation for this stemming function:\n",
    "http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/?fbclid=IwAR12cl7yIbpr-TXlS5IGukULm9kR30v_jocnRHAH5efFAECwTLlI7mt_yDw\n",
    "'''\n",
    "from textblob import TextBlob\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 µs, sys: 1 µs, total: 30 µs\n",
      "Wall time: 34.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigram_vect = CountVectorizer(stop_words = 'english', tokenizer = textblob_tokenizer, \n",
    "                              ngram_range=(1, 2), max_features = 15000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 42s, sys: 1.39 s, total: 3min 43s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_trainval_bigram = bigram_vect.fit_transform(text_X_trainval)\n",
    "X_test_bigram = bigram_vect.transform(text_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram text-based Rigde model score: 0.68\n",
      "Best param: {'regressor__alpha': 100}\n",
      "CPU times: user 4min 27s, sys: 1.82 s, total: 4min 29s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_trainval_bigram, y_trainval)\n",
    "\n",
    "print((\"N-gram text-based Rigde model score: %.2f\"\n",
    "       %grid.score(X_test_bigram, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Supplement the N-gram model with TF-IDF stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram text-based Ridge with TF-IDF stemming score: 0.69\n",
      "Best param: {'regressor__alpha': 1}\n",
      "CPU times: user 5min, sys: 2.15 s, total: 5min 2s\n",
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigram_tfidf = make_pipeline(bigram_vect,\n",
    "                             TfidfTransformer(),\n",
    "                            )\n",
    "\n",
    "X_trainval_bigram_tfidf = bigram_tfidf.fit_transform(text_X_trainval)\n",
    "\n",
    "X_test_bigram_tfidf = bigram_tfidf.transform(text_X_test)\n",
    "\n",
    "ridge_pipeline = Pipeline(steps=[('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(X_trainval_bigram_tfidf, y_trainval)\n",
    "\n",
    "print((\"N-gram text-based Ridge with TF-IDF stemming score: %.2f\"\n",
    "       %grid.score(X_test_bigram_tfidf, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "- Using n-grams with stop words improves the model's performance considerable (about 8%). We believe this performs better than a BoW model because it captures combinations of words associated with higher/lower scores. \n",
    "- Supplementing the n-gram model with TF-IDF stemming improves the performance even more (about 1%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4: Combining Text and Non-Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use similar preprocessing as the one used earlier with standard scaling of continuous features, OHE for categorical features and imputation of all missing values.\n",
    "- In addition to a RidgeRegressor, we tried XGBRegressor as well see if there was a considerable difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert sparse matrix to df\n",
    "X_trainval_bigram_tfidf = pd.DataFrame(X_trainval_bigram_tfidf.toarray())\n",
    "X_test_bigram_tfidf = pd.DataFrame(X_test_bigram_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 27 s, total: 2min 1s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Merge non-text and BOW and test\n",
    "full_X_trainval = pd.concat([non_text_X_trainval, X_trainval_bigram_tfidf], axis=1)\n",
    "full_X_test = pd.concat([non_text_X_test, X_test_bigram_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_categorical_transformer = Pipeline(steps=[('simpleimputer', SimpleImputer(strategy = 'constant', fill_value = \"missing\")),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "base_continuous_transformer = Pipeline(steps=[('simpleimputer', SimpleImputer(strategy = \"median\")),\n",
    "                                         ('scaler', StandardScaler())])\n",
    "\n",
    "base_continuous_and_categorical_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', base_continuous_transformer, nt_continuous_features),\n",
    "        ('cat', base_categorical_transformer, nt_categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 38.5 s, total: 1min 2s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformed_full_X_trainval = base_continuous_and_categorical_preprocessor.fit_transform(full_X_trainval)\n",
    "transformed_full_X_test = base_continuous_and_categorical_preprocessor.transform(full_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text & Non-Text] X_trainval:  (97478, 140635)\n",
      "[Text & Non-Text] X_test:  (32493, 140635)\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"[Text & Non-Text] X_trainval: \", transformed_full_X_trainval.shape)\n",
    "print(\"[Text & Non-Text] X_test: \",transformed_full_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text and non-text hybrid Ridge model score 0.49\n",
      "Best param: {'regressor__alpha': 0.01}\n",
      "CPU times: user 12min 8s, sys: 5.15 s, total: 12min 13s\n",
      "Wall time: 13min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(transformed_full_X_trainval, y_trainval)\n",
    "\n",
    "print((\"Text and non-text hybrid Ridge model score %.2f\"\n",
    "       %grid.score(transformed_full_X_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost with GridSearchCV 0.47\n",
      "Best param: {'regressor__alpha': 2, 'regressor__lambda': 0.5, 'regressor__max_depth': 8}\n",
      "CPU times: user 27min 44s, sys: 15.4 s, total: 28min\n",
      "Wall time: 34min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', XGBRegressor())])\n",
    "\n",
    "param_grid =  {\n",
    "               \"regressor__max_depth\": [4,8], \n",
    "                \"regressor__alpha\": [0, 0.5, 2], \n",
    "                \"regressor__lambda\": [0.5, 1.5],\n",
    "              }\n",
    "grid = GridSearchCV(xgb_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(transformed_full_X_trainval, y_trainval)\n",
    "\n",
    "print((\"XGBoost with GridSearchCV %.2f\"\n",
    "       %grid.score(transformed_full_X_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "- Combining text and non-text features lowers the performance for both Ridge and XGBRegressor considerably as opposed to a pure text-based model (approx. 17%). \n",
    "- XGBoost is also far slower to train than Ridge (approx. 13x longer) in addition to performing poorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
