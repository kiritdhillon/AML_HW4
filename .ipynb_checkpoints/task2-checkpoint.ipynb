{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:30px; text-align:center; line-height:120%\">\n",
    "    <br> \n",
    "        <b>\n",
    "        COMS 4995 Applied ML\n",
    "            Homework 4 \n",
    "        <br></br>\n",
    "            Predicting Wine Quality: Task 2\n",
    "        <br></br>\n",
    "        </b> \n",
    "    <br> \n",
    "</p>\n",
    "<p style=\"font-size:18px; text-align:left; line-height:120%\">\n",
    "    <br> \n",
    "        <b>\n",
    "        Kirit Dhillon, Sagar Lal\n",
    "        </b>\n",
    "    <br> \n",
    "        <b>\n",
    "        Uni: ksd2142, sl3946\n",
    "        </b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loanding and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"winemag-data-130k-v2.csv\")\n",
    "# Remove uninformative columns like \"Taster Name\" and \"Taster Twitter Handle\"\n",
    "data = data.drop(['taster_name', 'taster_twitter_handle'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data['description'], data['points'], stratify= data['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainval: \t (97478,) (97478,)\n",
      "X_test: \t (32493,) (32493,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_trainval: \\t\", X_trainval.shape, y_trainval.shape)\n",
    "print(\"X_test: \\t\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vect = CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    "\n",
    "X_trainval_bow = bow_vect.fit_transform(X_trainval)\n",
    "X_test_bow = bow_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BoW Text] X_trainval: \t (97478, 1000)\n",
      "[BoW Text] X_test:\t (32493, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "print(\"[BoW Text] X_trainval: \\t\", X_trainval_bow.shape)\n",
    "print(\"[BoW Text] X_test:\\t\", X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Word Embeddings Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We decided to use a pre-trained Doc2vec model called \"Associated Press News DBOW\" which is analogous to a skip-gram model in word2vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph. (Source: https://ibm.ent.box.com/s/9ebs3c759qqo1d8i7ed323i6shv2js7e)\n",
    "- We picked this over a word2vec model because it's far more efficient and computationally inexpensive for a large corpus like our wine review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec\n",
    "import gensim.models as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "model = g.Doc2Vec.load('./apnews_dbow/doc2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset so it can be processed by the doc2vec model\n",
    "def preprocess(model, dataset):\n",
    "    list_dataset= list(dataset.str.split(\" \", expand = False))\n",
    "    print(\"Successful splitting\")\n",
    "    w2v_dataset = []\n",
    "    j = 0\n",
    "    print(list_dataset[1])\n",
    "    for i in list_dataset:\n",
    "        j+=1\n",
    "        if j%1000 == 0:\n",
    "            print(j)\n",
    "        w2v_dataset.append(model.infer_vector(i))\n",
    "    return w2v_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful splitting\n",
      "['Soft,', 'ripe', 'and', 'round,', 'this', 'has', 'a', 'slightly', 'liquorous', 'bite', 'to', 'the', 'aromatics.', 'Pear', 'and', 'nectarine', 'fruit', 'is', 'accented', 'with', 'a', 'dash', 'of', 'cinnamon.', 'The', 'wine', 'was', '20%', 'barrel', 'fermented.']\n"
     ]
    }
   ],
   "source": [
    "w2v_x_test = preprocess(model, X_test)\n",
    "#print(\"Completed X_test preprocessing\")\n",
    "#w2v_x_trainval = preprocess(model, X_trainval)\n",
    "#print(\"Completed X_trainval preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to Pandas dataframe\n",
    "w2v_x_test = pd.DataFrame(w2v_x_test)\n",
    "w2v_x_trainval = pd.DataFrame(w2v_x_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97478, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debugging: \n",
    "print(\"[w2v] X_trainval: \\t\", w2v_x_trainval)\n",
    "print(\"[w2v] X_test: \\t\", w2v_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Ridge score with GridSearchCV 0.45\n",
      "Best param: {'regressor__alpha': 10}\n",
      "CPU times: user 12.9 s, sys: 3.99 s, total: 16.9 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, .1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(w2v_x_trainval, y_trainval)\n",
    "\n",
    "print((\"Doc2Vec Ridge score with GridSearchCV %.2f\"\n",
    "       %grid.score(w2v_x_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', XGBRegressor())])\n",
    "\n",
    "param_grid =  {\n",
    "               \"regressor__max_depth\": [4], \n",
    "                \"regressor__alpha\": [0], \n",
    "                \"regressor__lambda\": [0.5],\n",
    "              }\n",
    "grid = GridSearchCV(xgb_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(w2v_x_trainval, y_trainval)\n",
    "\n",
    "print((\"XGBoost with GridSearchCV %.2f\"\n",
    "       %grid.score(w2v_x_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis: \n",
    "- We tried the pre-trained Doc2Vec word embeddings for two models: Ridge and XGBRegressor\n",
    "- Both perform signficantly worse than models in task 1 (approx. 24%).\n",
    "- We believe that pre-trained word embeddings are not appropriate for featurization for this task because ________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Doc2Vec with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval_bow = pd.DataFrame(X_trainval_bow.toarray())\n",
    "X_test_bow = pd.DataFrame(X_test_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X_trainval = pd.concat([X_trainval_bow, w2v_x_trainval], axis=1)\n",
    "full_X_test = pd.concat([X_test_bow, w2v_x_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Ridge score with GridSearchCV 0.64\n",
      "Best param: {'regressor__alpha': 10}\n",
      "CPU times: user 2min 2s, sys: 37.7 s, total: 2min 40s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "                                ('regressor', Ridge())])\n",
    "\n",
    "param_grid =  {\n",
    "               'regressor__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "              }\n",
    "grid = GridSearchCV(ridge_pipeline, param_grid, cv=3, return_train_score=True)\n",
    "grid.fit(full_X_trainval, y_trainval)\n",
    "\n",
    "print((\"Baseline Ridge score with GridSearchCV %.2f\"\n",
    "       %grid.score(full_X_test, y_test)))\n",
    "print(\"Best param:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "- Combining Doc2Vec with BoW for featurization on Ridge performs very well, second only to the text-based N-gram text-based Ridge with TF-IDF stemming by 4% accuracy.\n",
    "- While this improves performance compared to solely Doc2Vec featurization, we find it's still better to avoid pre-trained embeddings altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
